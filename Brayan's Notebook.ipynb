{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descenso de la Gradiente Estocástico (Stochastic Gradient Descent)\n",
    "\n",
    "Es un algoritmo de optimización de primer orden y una variación del algoritmo de descenso de la gradiente (Gradient Descent).\n",
    "\n",
    "Se utiliza para aprender un conjunto de coeficientes de clasificación para el aprendizaje parametrizado [1], implementado sobretodo en conjuntos grandes de datos, pues este es más rápido en comparación a otros métodos matemáticos como el método estándar de gradient descent, además de que a su misma vez se evita realizar desperdicios computacionales.\n",
    "\n",
    "La diferencia que presenta con respecto al método de descenso de la gradiente es que se actualiza la matriz de pesos W en pequeños conjuntos de o arreglos de datos de entrenamiento.\n",
    "\n",
    "Este algoritmo permite realizar más pasos sobre el gradiente lo cual implica un paso por lote en comparación a un paso por época, lo que permite una convergencia más rápida que no va a afectar la precisión ni la pérdida de la clasificación.\n",
    "\n",
    "El objetivo de desarrollo de este método es evitar lo que ocurría en grandes conjuntos de datos con el algoritmo de descenso de gradiente, en el que para cada iteración de descenso de gradiente se requiere que se calcule una predicción para cada punto de entrenamiento del conjunto de datos de entrenamiento, esto se traduce a que por el cálculo de predicciones para cada punto se debe realizar la actualización de la matriz de pesos W, lo cual es un desperdicio computacional.\n",
    "\n",
    "\n",
    "Ejecución del\n",
    "\n",
    "Ref.\n",
    "[1] https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo de Código en Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente ejemplo muestra la ejecución del la búsqueda de un punto mínimo en un conjunto de  datos con el algoritmo de Descenso de la Gradiente Estocástico.\n",
    "\n",
    "* Ratio de Aprendizaje Establecido: 0.01\n",
    "    \n",
    "* Cantidad de Desplazamiento (epochs) establecidos: 100\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, lrate=0.050, error=956.079\n",
      ">epoch=1, lrate=0.050, error=619.540\n",
      ">epoch=2, lrate=0.050, error=242.444\n",
      ">epoch=3, lrate=0.050, error=276.882\n",
      ">epoch=4, lrate=0.050, error=246.313\n",
      ">epoch=5, lrate=0.050, error=231.177\n",
      ">epoch=6, lrate=0.050, error=214.019\n",
      ">epoch=7, lrate=0.050, error=198.366\n",
      ">epoch=8, lrate=0.050, error=183.468\n",
      ">epoch=9, lrate=0.050, error=169.456\n",
      ">epoch=10, lrate=0.050, error=156.280\n",
      ">epoch=11, lrate=0.050, error=143.927\n",
      ">epoch=12, lrate=0.050, error=132.380\n",
      ">epoch=13, lrate=0.050, error=121.619\n",
      ">epoch=14, lrate=0.050, error=111.627\n",
      ">epoch=15, lrate=0.050, error=102.387\n",
      ">epoch=16, lrate=0.050, error=93.882\n",
      ">epoch=17, lrate=0.050, error=86.095\n",
      ">epoch=18, lrate=0.050, error=79.010\n",
      ">epoch=19, lrate=0.050, error=72.612\n",
      ">epoch=20, lrate=0.050, error=66.883\n",
      ">epoch=21, lrate=0.050, error=61.810\n",
      ">epoch=22, lrate=0.050, error=57.376\n",
      ">epoch=23, lrate=0.050, error=53.568\n",
      ">epoch=24, lrate=0.050, error=50.370\n",
      ">epoch=25, lrate=0.050, error=47.768\n",
      ">epoch=26, lrate=0.050, error=45.748\n",
      ">epoch=27, lrate=0.050, error=44.296\n",
      ">epoch=28, lrate=0.050, error=43.399\n",
      ">epoch=29, lrate=0.050, error=43.044\n",
      ">epoch=30, lrate=0.050, error=43.217\n",
      ">epoch=31, lrate=0.050, error=43.906\n",
      ">epoch=32, lrate=0.050, error=45.098\n",
      ">epoch=33, lrate=0.050, error=46.781\n",
      ">epoch=34, lrate=0.050, error=48.943\n",
      ">epoch=35, lrate=0.050, error=51.572\n",
      ">epoch=36, lrate=0.050, error=54.656\n",
      ">epoch=37, lrate=0.050, error=58.185\n",
      ">epoch=38, lrate=0.050, error=62.146\n",
      ">epoch=39, lrate=0.050, error=66.530\n",
      ">epoch=40, lrate=0.050, error=71.324\n",
      ">epoch=41, lrate=0.050, error=76.519\n",
      ">epoch=42, lrate=0.050, error=82.105\n",
      ">epoch=43, lrate=0.050, error=88.071\n",
      ">epoch=44, lrate=0.050, error=94.407\n",
      ">epoch=45, lrate=0.050, error=101.103\n",
      ">epoch=46, lrate=0.050, error=108.150\n",
      ">epoch=47, lrate=0.050, error=115.539\n",
      ">epoch=48, lrate=0.050, error=123.260\n",
      ">epoch=49, lrate=0.050, error=131.305\n",
      ">epoch=50, lrate=0.050, error=139.664\n",
      ">epoch=51, lrate=0.050, error=148.329\n",
      ">epoch=52, lrate=0.050, error=157.291\n",
      ">epoch=53, lrate=0.050, error=166.542\n",
      ">epoch=54, lrate=0.050, error=176.074\n",
      ">epoch=55, lrate=0.050, error=185.878\n",
      ">epoch=56, lrate=0.050, error=195.947\n",
      ">epoch=57, lrate=0.050, error=206.274\n",
      ">epoch=58, lrate=0.050, error=216.850\n",
      ">epoch=59, lrate=0.050, error=227.669\n",
      ">epoch=60, lrate=0.050, error=238.722\n",
      ">epoch=61, lrate=0.050, error=250.003\n",
      ">epoch=62, lrate=0.050, error=261.506\n",
      ">epoch=63, lrate=0.050, error=273.222\n",
      ">epoch=64, lrate=0.050, error=285.146\n",
      ">epoch=65, lrate=0.050, error=297.270\n",
      ">epoch=66, lrate=0.050, error=309.589\n",
      ">epoch=67, lrate=0.050, error=322.096\n",
      ">epoch=68, lrate=0.050, error=334.785\n",
      ">epoch=69, lrate=0.050, error=347.650\n",
      ">epoch=70, lrate=0.050, error=360.685\n",
      ">epoch=71, lrate=0.050, error=373.884\n",
      ">epoch=72, lrate=0.050, error=387.241\n",
      ">epoch=73, lrate=0.050, error=400.751\n",
      ">epoch=74, lrate=0.050, error=414.409\n",
      ">epoch=75, lrate=0.050, error=428.209\n",
      ">epoch=76, lrate=0.050, error=442.146\n",
      ">epoch=77, lrate=0.050, error=456.214\n",
      ">epoch=78, lrate=0.050, error=470.409\n",
      ">epoch=79, lrate=0.050, error=484.727\n",
      ">epoch=80, lrate=0.050, error=499.161\n",
      ">epoch=81, lrate=0.050, error=513.707\n",
      ">epoch=82, lrate=0.050, error=528.361\n",
      ">epoch=83, lrate=0.050, error=543.119\n",
      ">epoch=84, lrate=0.050, error=557.975\n",
      ">epoch=85, lrate=0.050, error=572.925\n",
      ">epoch=86, lrate=0.050, error=587.966\n",
      ">epoch=87, lrate=0.050, error=603.093\n",
      ">epoch=88, lrate=0.050, error=618.302\n",
      ">epoch=89, lrate=0.050, error=633.589\n",
      ">epoch=90, lrate=0.050, error=648.950\n",
      ">epoch=91, lrate=0.050, error=664.381\n",
      ">epoch=92, lrate=0.050, error=679.879\n",
      ">epoch=93, lrate=0.050, error=695.440\n",
      ">epoch=94, lrate=0.050, error=711.061\n",
      ">epoch=95, lrate=0.050, error=726.738\n",
      ">epoch=96, lrate=0.050, error=742.467\n",
      ">epoch=97, lrate=0.050, error=758.246\n",
      ">epoch=98, lrate=0.050, error=774.070\n",
      ">epoch=99, lrate=0.050, error=789.938\n",
      "[19.145651470314025, -2.183175983637091]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def predict(row, coefficients):\n",
    "\tyhat = coefficients[0]\n",
    "\tfor i in range(len(row)-1):\n",
    "\t\tyhat += coefficients[i + 1] * row[i]\n",
    "\treturn yhat\n",
    " \n",
    "# Estimate linear regression coefficients using stochastic gradient descent\n",
    "def coefficients_sgd(train, l_rate, n_epoch):\n",
    "\tcoef = [0.0 for i in range(len(train[0]))]\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tsum_error = 0\n",
    "\t\tfor row in train:\n",
    "\t\t\tyhat = predict(row, coef)\n",
    "\t\t\terror = yhat - row[-1]\n",
    "\t\t\tsum_error += error**2\n",
    "\t\t\tcoef[0] = coef[0] - l_rate * error\n",
    "\t\t\tfor i in range(len(row)-1):\n",
    "\t\t\t\tcoef[i + 1] = coef[i + 1] - l_rate * error * row[i]\n",
    "\t\tprint('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "\treturn coef\n",
    " \n",
    "# Calculate coefficients\n",
    "dataset = [[random.randint(1,10), random.randint(1,10)], [random.randint(1,10), random.randint(1,10)], [random.randint(1,10), random.randint(1,10)],\n",
    "           [random.randint(1,10), random.randint(1,10)], [random.randint(1,10), random.randint(1,10)]]\n",
    "l_rate = 0.05\n",
    "n_epoch = 100\n",
    "coef = coefficients_sgd(dataset, l_rate, n_epoch)\n",
    "print(coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Código fuente y resultado de ejecución del algoritmo en Python*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
